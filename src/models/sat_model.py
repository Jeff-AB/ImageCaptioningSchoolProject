"""
This module contains the Show, Attend, and Tell model. This model
consists of encoder and decoder stages. The encoder is a 
convolutional neural netowrk, and the decodedr is an LSTM model
where the annotation vectors generated by the encoder is weighted
by an attention model.

By the end of this project, this module should contain the following

 - Encoder Module
 - Decoder Module
 - Bayesian Decoder Module
"""

import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import typing
from typing import Optional
from .utils import ModelComposition

class SATAttention(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        raise NotImplementedError
    def forward(self, x):
        raise NotImplementedError

class SATEncoder(nn.Module):
    """Show, Attend, and Tell encoder. For this project, we will use EfficientNet with ImageNet weights
    
    This part of the model is fairly simple, we take the convolutional outputs of the feature extraction 
    network, then resize it to the required size for the decoder. Given the small size of the dataset,
    it is prudent here to use a pretrained model, and remove the linear layers. 
    """
    def __init__(
        self,
        latent_vector_size: int,
        pretrained: bool = True,
        freeze: bool = True,
        unfreeze_last: int = 0,
    ) -> typing.NoReturn:
        super(SATEncoder, self).__init__()
        features = models.efficientnet_b0(pretrained=pretrained)

        # remove classifier at the top of the model
        features = nn.Sequential(*(list(features.children())[:-1]))

        # freeze model parameters
        if freeze:
            for param in features.parameters():
                param.requires_grad = False

        # only useful if model is already frozen. Unfreezes the last n layers
        if unfreeze_last > 0:
            for param in features.features[unfreeze_last].parameters():
                param.requires_grad = True
        self.features = features

    def forward(self, x):
        x = self.features(x)
        x = self.latent_vector(x)
        x = F.relu(x)
        return x


class SATDecoder(nn.Module):
    """Show, Attend, and Tell Decoder. For this we use an LSTM model to process the features
    
    This part of the model requires some additional work. According to the Show, Attend, and Tell paper,
    the decoder is composed of multiple parts: an MLP for initializing :math:`h_0`, an MLP for initializing :math:`c_0`, 
    an attention model, and a LSTM. To implement these components, we use a single Linear layer to represent the MLPs 
    encoding the initial hidden and memory state, an attention model following the specifications of the paper,
    and a LSTM Cell.  
    
    """

    def __init__(self, encoder_size:int, embedding_size:int, vocabulary_size:int, hidden_size:int, attention: Optional[nn.Module] = None, dropout_rate:float=0.5) -> typing.NoReturn:
        super().__init__()

        #Initializer MLPs
        self.fh = nn.Linear(256,256)
        self.fc = nn.Linear(256,256)
        self.fÎ² = nn.Linear(256, 256)
    
    def initialize_weights(self):
        raise NotImplementedError
    def initialize_hidden_states(self, encoded):
        mean =  encoded.mean(dim=1) # row wise mean to get the average a_i vector
        h = self.fh(mean)
        c = self.fc(mean)
        return h, c
    
    def forward(self, x):
        return self.model(x)


class BayesianSATDecoder(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        raise NotImplementedError

    def forward(self, x):
        raise NotImplementedError

