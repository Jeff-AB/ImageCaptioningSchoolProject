# -*- coding: utf-8 -*-
"""Metrics_for_ImageCaption.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WJTEAigkRHa-YHC00GuZzW928gQvS2y7
"""

def Calculate_metrics(reference, model_out):

  """
  Bilingual Evaluation Understudy (BLEU)
  BLEU compares the n-gram of the model output translation with n-gram of the reference translation to count the number of matches. 
  These matches are independent of the positions where they occur.
  """
  from nltk.translate.bleu_score import sentence_bleu
  bleu_reference = reference.split()
  bleu_model_out = model_out.split()
  print('Individual 1-gram: %f' % sentence_bleu(reference, model_out, weights=(1, 0, 0, 0)))
  print('Individual 2-gram: %f' % sentence_bleu(reference, model_out, weights=(0, 1, 0, 0)))
  print('Individual 3-gram: %f' % sentence_bleu(reference, model_out, weights=(0, 0, 1, 0)))
  print('Individual 4-gram: %f' % sentence_bleu(reference, model_out, weights=(0, 0, 0, 1)))
  
  """
  Recall-Oriented Understudy for Gisting Evaluation (ROUGE)
  ROUGE-N measures the number of matching ‘n-grams’ between our model-generated text and a ‘reference’.
  """
  #  pip install rouge
  from rouge import Rouge
  rouge = Rouge()
  print('ROUGE Score: %f' % rouge.get_scores(model_out, reference))

  """
  Metric for Evaluation of Translation with Explicit ORdering (METEOR):

  Unigram precision P is calculated as:
  P = m/ w_t
  Unigram recall R is computed as:
  R = m/w_r
  where,
    m   : number of unigrams in the Model output translation that are also found in the reference translation
    w_t : the number of unigrams in the candidate translation
    w_r : number of unigrams in the reference translation

  Precision and recall are combined using the harmonic mean:
  F_mean = 10PR/(R+9P)

  The penalty p is computed as:
  p = 0.5(c/u_m)^3
  where,
    c   : number of chunks
    u_m : number of unigrams that have been mapped
  
  METEOR score:
  M = F_mean(1-p)
  """
  meteor_score = nltk.translate.meteor_score(model_out, reference)
  print('ROUGE Score: %f' % meteor_score)
